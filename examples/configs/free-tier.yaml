# Free-Tier Deployment Mode
# Uses only free-tier models and providers
# Ideal for: Open-source projects, learning, personal use, cost-free development

mode: free_only

# Provider configuration
providers:
  # Primary: Groq (generous free tier)
  - type: groq
    enabled: true
    priority: 10
    apiKey: ${GROQ_API_KEY}
    baseUrl: https://api.groq.com/openai/v1
    defaultModel: llama-3.3-70b-versatile

    # Free models only
    modelOverrides:
      test-generation: llama-3.3-70b-versatile
      code-review: llama-3.1-70b-versatile
      documentation: llama3-8b-8192
      refactoring: llama-3.3-70b-versatile
      coverage-analysis: llama3-8b-8192
      bug-detection: llama-3.1-70b-versatile

    costPer1MTokens:
      input: 0.59
      output: 0.79

    # Respect free tier limits
    limits:
      requestsPerMinute: 30
      requestsPerDay: 14400
      tokensPerMinute: 20000
      tokensPerDay: 1000000

    fallbackProvider: google

  # Secondary: Google Gemini (free tier)
  - type: google
    enabled: true
    priority: 20
    apiKey: ${GOOGLE_API_KEY}
    baseUrl: https://generativelanguage.googleapis.com/v1
    defaultModel: gemini-2.0-flash-exp

    # Free Gemini models
    modelOverrides:
      test-generation: gemini-2.0-flash-exp
      code-review: gemini-2.0-flash-exp
      documentation: gemini-1.5-flash
      coverage-analysis: gemini-1.5-flash

    costPer1MTokens:
      input: 0  # Free tier
      output: 0

    # Gemini free tier limits
    limits:
      requestsPerMinute: 15
      requestsPerDay: 1500

    fallbackProvider: github

  # Tertiary: GitHub Models (free for GitHub users)
  - type: github
    enabled: true
    priority: 30
    apiKey: ${GITHUB_TOKEN}
    baseUrl: https://models.inference.ai.azure.com
    defaultModel: gpt-4o

    # Free models via GitHub
    modelOverrides:
      code-review: gpt-4o
      test-generation: gpt-4o-mini
      documentation: gpt-4o-mini

    costPer1MTokens:
      input: 0  # Free for GitHub users
      output: 0

    # GitHub Models limits
    limits:
      requestsPerMinute: 10
      requestsPerDay: 500

  # Quaternary: Ollama (local, completely free)
  - type: ollama
    enabled: true
    priority: 40
    baseUrl: http://localhost:11434
    defaultModel: llama3.2:3b

    # Lightweight local models
    modelOverrides:
      documentation: llama3.2:3b
      coverage-analysis: llama3.2:3b
      bug-detection: qwen2.5:7b

    costPer1MTokens:
      input: 0
      output: 0

# Fallback chain: Groq -> Google -> GitHub -> Ollama
fallbackChain:
  - groq
  - google
  - github
  - ollama

# No cost budget needed (all free)
enableCostTracking: false

# Health monitoring (check frequently as free tiers can be rate-limited)
enableHealthChecks: true
healthCheckInterval: 60000  # 1 minute
maxConsecutiveFailures: 2

# Retry configuration (aggressive retries for rate limits)
enableRetries: true
maxRetries: 3

# Logging
logLevel: info

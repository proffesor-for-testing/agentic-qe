# ruvllm Integration GOAP Plan for Agentic QE Fleet

**Document Version:** 1.0.0
**Created:** 2025-12-04
**Project:** Agentic QE Fleet v2.1.0
**Location:** /workspaces/agentic-qe-cf

---

## Executive Summary

This document outlines a Goal-Oriented Action Planning (GOAP) strategy for integrating ruvllm (local LLM inference) into the Agentic QE Fleet to enhance agent capabilities with offline test generation, faster pattern recognition, cost reduction, privacy-sensitive code analysis, and edge/CI environment execution.

**Key Goals:**
1. Enable offline LLM inference for QE agents
2. Reduce API costs by 60-80% through local model usage
3. Improve response latency by 3-5x for pattern-matching tasks
4. Enable privacy-preserving code analysis for sensitive codebases
5. Support edge/CI environments without external API dependencies

---

## Part 1: Current State Assessment

### 1.1 Project Structure

**Location:** `/workspaces/agentic-qe-cf`

**Key Components:**
- **18 QE Agents:** Test generation, coverage analysis, performance, security, flaky detection
- **11 QE Subagents:** TDD specialists (RED/GREEN/REFACTOR), code reviewers, integration testers
- **41 QE Skills:** Including agentic-quality-engineering, tdd-london-chicago, api-testing-patterns
- **8 Slash Commands:** `/aqe-execute`, `/aqe-generate`, `/aqe-coverage`, `/aqe-quality`

**Technology Stack:**
- **Runtime:** Node.js 20+, TypeScript 5.9.3
- **LLM Providers:**
  - Anthropic SDK v0.64.0 (`@anthropic-ai/sdk`)
  - OpenAI SDK v6.9.1
- **Orchestration:** Claude Flow (hierarchical topology, max 10 agents)
- **Memory:** AgentDB v1.6.1, ruvector v0.1.24 for vector storage
- **Learning:** Q-learning, performance tracking, pattern extraction
- **Observability:** OpenTelemetry, WebSocket streaming, Grafana dashboards

### 1.2 Current LLM Integration Patterns

**API-Based Inference:**
```typescript
// Current pattern in src/utils/prompt-cache.ts
import Anthropic from '@anthropic-ai/sdk';

class PromptCacheManager {
  private anthropic: Anthropic;

  constructor(apiKey: string) {
    this.anthropic = new Anthropic({ apiKey });
  }

  async makeRequest(model: string, messages: any[]) {
    return await this.anthropic.messages.create({
      model,
      messages,
      // ... caching config
    });
  }
}
```

**Model Router Architecture:**
```typescript
// src/core/routing/AdaptiveModelRouter.ts
export enum AIModel {
  CLAUDE_SONNET_4_5 = 'claude-sonnet-4.5',
  CLAUDE_HAIKU = 'claude-haiku',
  // ... other models
}

class AdaptiveModelRouter {
  async selectModel(task: QETask): Promise<ModelSelection> {
    const analysis = this.complexityAnalyzer.analyzeComplexity(task);
    const model = this.selectModelForTask(agentType, analysis.complexity);
    const estimatedCost = await this.estimateCost(model, analysis.estimatedTokens);
    // Returns optimal model based on cost/performance
  }
}
```

**Key Observations:**
- âœ… Well-architected model routing system with complexity analysis
- âœ… Cost tracking and optimization (70-81% savings via multi-model routing)
- âœ… Prompt caching infrastructure (90% discount on cached tokens)
- âŒ **No local inference support** - all requests go to external APIs
- âŒ **No offline capability** - requires network connectivity
- âŒ **Limited privacy controls** - code sent to external providers
- âŒ **CI/CD bottleneck** - API rate limits affect parallel test generation

### 1.3 Agent Capabilities Analysis

**BaseAgent Architecture** (`src/agents/BaseAgent.ts`):
```typescript
export abstract class BaseAgent extends EventEmitter {
  protected readonly agentId: AgentId;
  protected readonly capabilities: Map<string, AgentCapability>;
  protected readonly memoryStore: MemoryStore;
  protected performanceTracker?: PerformanceTracker;
  protected learningEngine?: LearningEngine;
  protected agentDB?: AgentDBManager; // AgentDB integration

  // Lifecycle hooks
  protected hookManager: VerificationHookManager;
  protected lifecycleManager: AgentLifecycleManager;
  protected coordinator: AgentCoordinator;
  protected memoryService: AgentMemoryService;
}
```

**Agent Categories:**

1. **Core Testing Agents (5):**
   - `qe-test-generator`: AI-powered test generation with sublinear optimization
   - `qe-test-executor`: Multi-framework test execution with parallel processing
   - `qe-coverage-analyzer`: Real-time gap detection with O(log n) algorithms
   - `qe-quality-gate`: Intelligent quality gate with risk assessment
   - `qe-quality-analyzer`: Comprehensive quality metrics analysis

2. **Analysis Agents (6):**
   - `qe-flaky-test-hunter`: ML-based flaky test detection (100% accuracy)
   - `qe-performance-tester`: Load testing, profiling, benchmarking
   - `qe-security-scanner`: OWASP scanning, vulnerability detection
   - `qe-code-complexity`: Cyclomatic/cognitive complexity analysis
   - `qe-api-contract-validator`: OpenAPI/GraphQL contract validation
   - `qe-regression-risk-analyzer`: Risk-based regression analysis

3. **Intelligence Agents (4):**
   - `qe-production-intelligence`: Production monitoring and feedback
   - `qe-requirements-validator`: Requirements traceability
   - `qe-test-data-architect`: Test data generation and management
   - `qe-deployment-readiness`: Pre-deployment validation

4. **Coordination Agents (3):**
   - `qe-fleet-commander`: Fleet orchestration and coordination
   - `qe-learning-coordinator`: Cross-agent learning coordination
   - `qe-qx-partner`: Quality experience analysis (QX Partner)

**Learning System** (`src/learning/`):
- Q-learning based strategy optimization
- Pattern extraction and reuse (20% improvement target)
- Experience replay buffer
- Flaky test prediction with ML models
- Performance tracking and improvement loops

### 1.4 Current Limitations

**Performance Bottlenecks:**
1. **API Latency:** 200-800ms per LLM request (network + processing)
2. **Rate Limits:** Throttling during parallel test generation (CI/CD impact)
3. **Cost at Scale:** $0.003-$0.015 per 1K tokens (adds up with 18 agents)
4. **Dependency:** Requires internet connectivity and API keys

**Privacy & Security:**
1. **Code Exposure:** Source code sent to external providers
2. **Compliance Risk:** GDPR/HIPAA concerns for sensitive codebases
3. **Data Sovereignty:** No control over where data is processed

**Operational Constraints:**
1. **Edge Environments:** Cannot run in air-gapped/offline environments
2. **CI/CD Integration:** API costs discourage extensive test generation
3. **Development Workflow:** Developers hesitant to share proprietary code

---

## Part 2: Gap Analysis - Where ruvllm Can Help

### 2.1 High-Impact Opportunities

#### Opportunity 1: Offline Test Generation
**Current State:** Test generation requires API calls to Anthropic/OpenAI
**Target State:** Local LLM generates tests without external dependencies

**Impact:**
- âœ… Enable offline development workflows
- âœ… Support air-gapped environments
- âœ… Eliminate API rate limit bottlenecks
- âœ… Reduce test generation cost by 80%+

**Use Cases:**
- CI/CD pipelines running 100+ test generation tasks/day
- Developers working on flights, remote locations
- Enterprise environments with network restrictions

#### Opportunity 2: Fast Pattern Recognition
**Current State:** Pattern matching uses embedding generation via API
**Target State:** Local model performs semantic similarity in <50ms

**Impact:**
- âœ… 5-10x faster pattern matching (50ms vs 200-500ms)
- âœ… Real-time pattern suggestions during coding
- âœ… Reduced memory pressure (no API request queuing)

**Use Cases:**
- QEReasoningBank pattern matching (41 skills, 1000+ patterns)
- Real-time test suggestion in IDE
- Coverage gap analysis during development

#### Opportunity 3: Privacy-Preserving Code Analysis
**Current State:** Proprietary code sent to external LLM providers
**Target State:** Local inference keeps all code on-premise

**Impact:**
- âœ… GDPR/HIPAA compliance for healthcare, finance
- âœ… Enterprise adoption for proprietary codebases
- âœ… Government/defense contractor eligibility
- âœ… Customer trust and competitive advantage

**Use Cases:**
- Financial services testing (PCI-DSS compliance)
- Healthcare systems (HIPAA compliance)
- Defense contractors (ITAR compliance)

#### Opportunity 4: Cost Optimization at Scale
**Current State:** $0.003-$0.015 per 1K tokens across 18 agents
**Target State:** Fixed cost for local model + infrastructure

**Impact:**
- âœ… 60-80% cost reduction for high-volume users
- âœ… Predictable pricing model (capex vs opex)
- âœ… Enables aggressive test generation strategies

**Cost Model Comparison:**
```
API-based (current):
- 1M tokens/day Ã— 18 agents = 18M tokens/day
- 18M tokens Ã— $0.003/1K = $54/day = $1,620/month

Local inference (ruvllm):
- Infrastructure: $500/month (GPU server or amortized)
- Electricity: ~$50/month
- Total: $550/month = 66% savings
```

#### Opportunity 5: Edge/CI Environment Execution
**Current State:** API calls from CI require secrets management, networking
**Target State:** Local inference runs in isolated CI containers

**Impact:**
- âœ… Simplified CI/CD configuration (no API keys)
- âœ… Faster CI pipeline execution (no network latency)
- âœ… Better parallelization (no rate limits)
- âœ… Reduced attack surface (no external calls)

### 2.2 Technical Gaps

**Gap 1: Inference Engine Interface**
- No abstraction layer for LLM providers
- Direct coupling to Anthropic/OpenAI SDKs
- No pluggable model backend system

**Gap 2: Model Management**
- No local model download/caching system
- No model versioning or updates
- No fallback mechanism (local â†’ API)

**Gap 3: Performance Optimization**
- No batch inference support
- No request queuing for local models
- No GPU utilization tracking

**Gap 4: Quality Assurance**
- No validation that local models produce equivalent results
- No A/B testing framework (local vs API)
- No quality metrics for local inference

---

## Part 3: Prioritized Improvement Goals

### 3.1 Goal Hierarchy (GOAP Tree)

```
Root Goal: Integrate ruvllm for hybrid local/cloud LLM inference
â”œâ”€â”€ Goal 1: Create abstraction layer for LLM providers (Foundation)
â”‚   â”œâ”€â”€ Success Criteria: All agents use ILLMProvider interface
â”‚   â”œâ”€â”€ Priority: CRITICAL
â”‚   â””â”€â”€ Dependencies: None
â”‚
â”œâ”€â”€ Goal 2: Implement ruvllm inference provider (Core)
â”‚   â”œâ”€â”€ Success Criteria: Local inference works with Qwen2.5-Coder-7B
â”‚   â”œâ”€â”€ Priority: HIGH
â”‚   â””â”€â”€ Dependencies: Goal 1
â”‚
â”œâ”€â”€ Goal 3: Build hybrid routing system (Intelligence)
â”‚   â”œâ”€â”€ Success Criteria: Auto-select local vs cloud based on task
â”‚   â”œâ”€â”€ Priority: HIGH
â”‚   â””â”€â”€ Dependencies: Goals 1, 2
â”‚
â”œâ”€â”€ Goal 4: Enable privacy mode for sensitive code (Security)
â”‚   â”œâ”€â”€ Success Criteria: Force local-only inference option
â”‚   â”œâ”€â”€ Priority: MEDIUM
â”‚   â””â”€â”€ Dependencies: Goals 2, 3
â”‚
â””â”€â”€ Goal 5: Optimize for CI/CD environments (Performance)
    â”œâ”€â”€ Success Criteria: CI tests run 3x faster with local models
    â”œâ”€â”€ Priority: MEDIUM
    â””â”€â”€ Dependencies: Goals 2, 3, 4
```

### 3.2 Goal Definitions

#### Goal 1: Create Abstraction Layer for LLM Providers
**Objective:** Decouple agent logic from specific LLM provider implementations

**Success Criteria:**
- âœ… `ILLMProvider` interface with `complete()`, `embed()`, `stream()` methods
- âœ… `AnthropicProvider`, `OpenAIProvider`, `RuvllmProvider` implementations
- âœ… All 18 agents refactored to use interface (no direct SDK imports)
- âœ… Unit tests for provider switching
- âœ… Documentation for adding new providers

**Acceptance Tests:**
1. Can switch providers via config without code changes
2. Agents work with any provider implementation
3. Error handling consistent across providers

**Risk:** Medium (refactoring risk, but well-architected codebase)

---

#### Goal 2: Implement ruvllm Inference Provider
**Objective:** Enable local LLM inference via ruvllm for test generation

**Success Criteria:**
- âœ… `RuvllmProvider` implements `ILLMProvider` interface
- âœ… Supports Qwen2.5-Coder-7B-Instruct (code-optimized model)
- âœ… Model auto-download and caching (HuggingFace Hub)
- âœ… Streaming response support for real-time feedback
- âœ… GPU acceleration when available (CUDA/Metal)
- âœ… Graceful CPU fallback for resource-constrained environments

**Technical Requirements:**
```typescript
interface RuvllmConfig {
  modelPath: string;           // Path to GGUF model
  contextLength: number;        // Context window (default: 8192)
  temperature: number;          // Sampling temperature
  maxTokens: number;            // Max generation tokens
  gpuLayers?: number;           // GPU offload layers
  threads?: number;             // CPU threads
}

class RuvllmProvider implements ILLMProvider {
  async complete(prompt: string, options?: CompletionOptions): Promise<string>;
  async embed(text: string): Promise<number[]>;
  async stream(prompt: string, callback: StreamCallback): Promise<void>;
}
```

**Model Selection:**
- **Primary:** Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf (5.6GB)
  - Code-specialized (Java, Python, TS, etc.)
  - 7B parameters = good quality/speed tradeoff
  - Quantized Q5 = faster inference, reasonable quality

- **Fallback:** Phi-3-Mini-4K-Instruct (3.8B, 2.3GB)
  - Smaller footprint for resource-constrained CI
  - Still capable for test generation

**Risk:** Low (ruvllm is well-tested, good documentation)

---

#### Goal 3: Build Hybrid Routing System
**Objective:** Intelligently route requests between local and cloud models

**Success Criteria:**
- âœ… Extend `AdaptiveModelRouter` with local/cloud routing logic
- âœ… Route complex tasks to cloud (Claude Opus), simple to local
- âœ… Automatic fallback: local â†’ cloud on failure
- âœ… Cost tracking includes local infrastructure amortization
- âœ… Performance metrics: latency, throughput, accuracy

**Routing Logic:**
```typescript
enum InferenceMode {
  LOCAL_ONLY,     // Privacy mode
  CLOUD_ONLY,     // Legacy mode
  HYBRID_AUTO,    // Intelligent routing (default)
  HYBRID_PREFER_LOCAL  // Use local unless quality concern
}

interface RoutingDecision {
  provider: 'local' | 'cloud';
  model: string;
  reasoning: string;
  estimatedCost: number;
  estimatedLatency: number;
}

class HybridModelRouter extends AdaptiveModelRouter {
  async route(task: QETask): Promise<RoutingDecision> {
    // Route to local for:
    // - Test generation (well-structured)
    // - Pattern matching (fast, repetitive)
    // - Code analysis (privacy-sensitive)

    // Route to cloud for:
    // - Complex reasoning (security analysis)
    // - Novel scenarios (no pattern match)
    // - High-stakes decisions (quality gate)
  }
}
```

**Decision Factors:**
- **Task Complexity:** Cyclomatic complexity, LOC, novelty
- **Privacy Requirements:** Sensitive code â†’ local mandatory
- **Performance Needs:** Latency SLA â†’ local preferred
- **Quality Threshold:** Critical decisions â†’ cloud for safety
- **Cost Budget:** Cost-sensitive â†’ maximize local usage

**Risk:** Medium (requires careful tuning, A/B testing)

---

#### Goal 4: Enable Privacy Mode for Sensitive Code
**Objective:** Guarantee no code leaves customer infrastructure

**Success Criteria:**
- âœ… `--privacy-mode` flag forces local-only inference
- âœ… Audit logs prove no external API calls
- âœ… Configuration validation prevents cloud routing
- âœ… Documentation for compliance officers
- âœ… Performance acceptable for 90% of tasks

**Implementation:**
```typescript
interface PrivacyConfig {
  mode: 'strict' | 'balanced' | 'off';
  allowedProviders: string[];  // ['ruvllm'] for strict
  auditLog: boolean;           // Log all LLM requests
  blockExternalCalls: boolean; // Network firewall
}

class PrivacyModeGuard {
  validate(provider: string): void {
    if (this.config.mode === 'strict' && provider !== 'ruvllm') {
      throw new PrivacyViolationError('External LLM calls disabled');
    }
  }
}
```

**Compliance Features:**
- Audit trail: Log every LLM request (local/cloud)
- Network validation: Fail if cloud call attempted in strict mode
- Certification: Generate compliance reports (GDPR, HIPAA)

**Risk:** Low (straightforward guard logic)

---

#### Goal 5: Optimize for CI/CD Environments
**Objective:** Make CI pipelines 3x faster with local inference

**Success Criteria:**
- âœ… CI container with pre-warmed model (cold start <5s)
- âœ… Batch test generation: 10 files in <30s (vs 120s cloud)
- âœ… Parallel inference: Support 4+ concurrent requests
- âœ… Memory efficiency: <8GB RAM per container
- âœ… Docker images published to GitHub Container Registry

**CI Optimization Strategies:**

1. **Model Caching:**
```dockerfile
FROM node:20-slim
# Pre-download model during image build
RUN npx ruvllm download Qwen2.5-Coder-7B-Instruct-Q5_K_M
COPY . /app
WORKDIR /app
CMD ["aqe", "generate", "--mode", "local"]
```

2. **Warm Pool:**
- Keep model loaded in memory between test runs
- Shared model server for multiple CI jobs
- Kubernetes sidecar pattern

3. **Batch Inference:**
```typescript
class BatchInferenceQueue {
  private queue: InferenceRequest[] = [];

  async enqueue(request: InferenceRequest): Promise<string> {
    this.queue.push(request);
    if (this.queue.length >= 4) {
      return this.processBatch();
    }
  }

  async processBatch(): Promise<void> {
    // Process 4 requests in parallel
    const results = await Promise.all(
      this.queue.splice(0, 4).map(r => this.model.complete(r))
    );
  }
}
```

**Performance Targets:**
- Cold start: <5s (model load)
- Warm inference: <2s per test file
- Throughput: 20 tests/minute
- Memory: <8GB RAM

**Risk:** Medium (requires CI environment tuning)

---

## Part 4: Detailed Action Plan with Milestones

### Phase 1: Foundation (Weeks 1-2)

#### Milestone 1.1: LLM Provider Abstraction
**Duration:** 4 days
**Owner:** Core team
**Dependencies:** None

**Tasks:**
1. Design `ILLMProvider` interface
   - Methods: `complete()`, `embed()`, `stream()`, `chatComplete()`
   - Error handling: `LLMProviderError`, retry logic
   - Configuration: `LLMProviderConfig` interface

2. Create `src/core/llm/` module structure
   ```
   src/core/llm/
   â”œâ”€â”€ ILLMProvider.ts          # Interface definition
   â”œâ”€â”€ LLMProviderFactory.ts    # Provider registry
   â”œâ”€â”€ providers/
   â”‚   â”œâ”€â”€ AnthropicProvider.ts
   â”‚   â”œâ”€â”€ OpenAIProvider.ts
   â”‚   â””â”€â”€ RuvllmProvider.ts    # Placeholder
   â”œâ”€â”€ types.ts                 # Shared types
   â””â”€â”€ utils.ts                 # Prompt formatting
   ```

3. Implement `AnthropicProvider` (refactor existing)
   - Extract from `src/utils/prompt-cache.ts`
   - Maintain prompt caching support
   - Preserve cost tracking

4. Implement `OpenAIProvider` (minimal, for comparison)

5. Write unit tests (80%+ coverage)
   - Provider switching
   - Error handling
   - Configuration validation

**Deliverables:**
- âœ… `src/core/llm/` module with 4 providers (1 placeholder)
- âœ… Unit tests (Jest)
- âœ… Documentation: `docs/architecture/llm-providers.md`

**Acceptance Criteria:**
- Can instantiate any provider via factory
- Consistent error handling across providers
- Tests pass with 80%+ coverage

---

#### Milestone 1.2: Refactor BaseAgent to Use ILLMProvider
**Duration:** 3 days
**Owner:** Agent team
**Dependencies:** Milestone 1.1

**Tasks:**
1. Add `llmProvider` to `BaseAgentConfig`
   ```typescript
   export interface BaseAgentConfig {
     // ... existing fields
     llmProvider?: ILLMProvider; // Injected or default
   }
   ```

2. Refactor `TestGeneratorAgent` (pilot agent)
   - Replace direct Anthropic SDK calls
   - Use `this.llmProvider.complete()`
   - Maintain existing behavior (tests should still pass)

3. Refactor remaining 17 agents
   - Bulk refactoring script
   - Validate each agent with integration tests

4. Update `FleetCommanderAgent` to inject providers
   - Centralized provider configuration
   - Per-agent provider override support

**Deliverables:**
- âœ… All 18 agents use `ILLMProvider`
- âœ… Integration tests pass (no behavior changes)
- âœ… Migration guide: `docs/migration/llm-providers.md`

**Acceptance Criteria:**
- No direct imports of `@anthropic-ai/sdk` in agent files
- All existing tests pass
- Can switch providers via config

---

### Phase 2: Core Integration (Weeks 3-4)

#### Milestone 2.1: ruvllm Provider Implementation
**Duration:** 5 days
**Owner:** Infrastructure team
**Dependencies:** Milestone 1.2

**Tasks:**
1. Install ruvllm dependency
   ```bash
   npm install ruvllm
   ```

2. Implement `RuvllmProvider`
   ```typescript
   import { Ruvllm } from 'ruvllm';

   export class RuvllmProvider implements ILLMProvider {
     private model: Ruvllm;

     constructor(config: RuvllmConfig) {
       this.model = new Ruvllm({
         modelPath: config.modelPath || 'Qwen2.5-Coder-7B-Instruct-Q5_K_M',
         contextLength: config.contextLength || 8192,
         gpuLayers: config.gpuLayers || -1, // Auto-detect
       });
     }

     async complete(prompt: string, options?: CompletionOptions): Promise<string> {
       const response = await this.model.complete({
         prompt: this.formatPrompt(prompt, options),
         maxTokens: options?.maxTokens || 2048,
         temperature: options?.temperature || 0.7,
         stopSequences: options?.stopSequences,
       });
       return response.text;
     }

     async embed(text: string): Promise<number[]> {
       // Use ruvllm's embedding endpoint or fallback to ruvector
       return this.model.embed(text);
     }

     async stream(prompt: string, callback: StreamCallback): Promise<void> {
       const stream = await this.model.streamComplete({
         prompt: this.formatPrompt(prompt),
       });

       for await (const chunk of stream) {
         callback(chunk.text);
       }
     }
   }
   ```

3. Implement model download system
   ```typescript
   class ModelManager {
     async downloadModel(modelName: string): Promise<string> {
       // Use ruvllm's download API
       const path = await Ruvllm.download(modelName, {
         quantization: 'Q5_K_M',
         cache: true,
       });
       return path;
     }

     async listModels(): Promise<string[]> {
       // List cached models
     }
   }
   ```

4. Add GPU detection and optimization
   ```typescript
   function detectGPU(): { available: boolean; layers: number } {
     // Detect CUDA/Metal availability
     // Return optimal GPU layer count
   }
   ```

5. Write integration tests
   - Test basic completion
   - Test streaming
   - Test embedding generation
   - Test error handling (model not found, OOM)

**Deliverables:**
- âœ… `RuvllmProvider` implementation
- âœ… Model download CLI: `aqe model download <name>`
- âœ… Integration tests
- âœ… Docs: `docs/guides/local-inference.md`

**Acceptance Criteria:**
- Can generate tests with local model
- Inference completes in <3s for 500-token prompts
- GPU acceleration works when available
- Graceful CPU fallback

---

#### Milestone 2.2: Hybrid Routing Logic
**Duration:** 4 days
**Owner:** Routing team
**Dependencies:** Milestones 1.2, 2.1

**Tasks:**
1. Extend `AdaptiveModelRouter` with local/cloud routing
   ```typescript
   enum ProviderType {
     LOCAL = 'local',
     CLOUD = 'cloud',
   }

   interface HybridRoutingConfig extends RouterConfig {
     mode: InferenceMode;
     localProvider: ILLMProvider;
     cloudProvider: ILLMProvider;
     fallbackToCloud: boolean;
     routingRules: RoutingRule[];
   }

   class HybridModelRouter extends AdaptiveModelRouter {
     async selectProvider(task: QETask): Promise<ProviderSelection> {
       // Analyze task characteristics
       const analysis = this.analyzeTask(task);

       // Check privacy requirements
       if (this.config.mode === InferenceMode.LOCAL_ONLY) {
         return { provider: this.localProvider, type: ProviderType.LOCAL };
       }

       // Route based on complexity
       if (analysis.complexity > 0.7) {
         return { provider: this.cloudProvider, type: ProviderType.CLOUD };
       }

       // Prefer local for cost savings
       return { provider: this.localProvider, type: ProviderType.LOCAL };
     }
   }
   ```

2. Define routing rules
   ```typescript
   const ROUTING_RULES: RoutingRule[] = [
     {
       name: 'privacy-sensitive-code',
       condition: (task) => task.metadata?.privacy === 'strict',
       action: 'route-to-local',
       priority: 100, // Highest priority
     },
     {
       name: 'high-complexity-reasoning',
       condition: (task) => task.complexity > 0.8,
       action: 'route-to-cloud',
       priority: 80,
     },
     {
       name: 'cost-optimization',
       condition: (task) => task.type === 'test-generation',
       action: 'prefer-local',
       priority: 50,
     },
   ];
   ```

3. Implement fallback mechanism
   ```typescript
   async executeWithFallback(
     task: QETask,
     primaryProvider: ILLMProvider,
     fallbackProvider: ILLMProvider
   ): Promise<string> {
     try {
       return await primaryProvider.complete(task.prompt);
     } catch (error) {
       if (this.config.fallbackToCloud) {
         console.warn('Local inference failed, falling back to cloud');
         return await fallbackProvider.complete(task.prompt);
       }
       throw error;
     }
   }
   ```

4. Add telemetry
   - Track routing decisions (local vs cloud)
   - Measure latency per provider type
   - Calculate cost savings

**Deliverables:**
- âœ… `HybridModelRouter` implementation
- âœ… Routing rules configuration
- âœ… Telemetry integration
- âœ… Docs: `docs/architecture/hybrid-routing.md`

**Acceptance Criteria:**
- Routes 80% of test generation to local
- Routes security analysis to cloud
- Fallback works when local fails
- Telemetry shows routing breakdown

---

### Phase 3: Optimization (Weeks 5-6)

#### Milestone 3.1: Performance Optimization
**Duration:** 4 days
**Owner:** Performance team
**Dependencies:** Milestones 2.1, 2.2

**Tasks:**
1. Implement model warm pool
   ```typescript
   class ModelPool {
     private models: Map<string, Ruvllm> = new Map();

     async getModel(modelName: string): Promise<Ruvllm> {
       if (!this.models.has(modelName)) {
         const model = await this.loadModel(modelName);
         this.models.set(modelName, model);
       }
       return this.models.get(modelName)!;
     }

     async warmUp(): Promise<void> {
       // Pre-load models during startup
       await this.getModel('Qwen2.5-Coder-7B-Instruct-Q5_K_M');
     }
   }
   ```

2. Add batch inference queue
   ```typescript
   class InferenceQueue {
     private queue: QueuedRequest[] = [];
     private batchSize = 4;

     async enqueue(request: InferenceRequest): Promise<string> {
       return new Promise((resolve, reject) => {
         this.queue.push({ request, resolve, reject });
         if (this.queue.length >= this.batchSize) {
           this.processBatch();
         }
       });
     }
   }
   ```

3. Optimize prompt templates
   - Reduce token count for common prompts
   - Use prompt caching for system instructions
   - Template library for test generation

4. GPU utilization monitoring
   ```typescript
   interface GPUMetrics {
     utilization: number;  // 0-100%
     memoryUsed: number;   // GB
     memoryTotal: number;  // GB
     temperature: number;  // Celsius
   }

   class GPUMonitor {
     async getMetrics(): Promise<GPUMetrics> {
       // Query NVIDIA SMI or Metal stats
     }
   }
   ```

5. Benchmark suite
   - Compare local vs cloud latency
   - Measure throughput (tests/minute)
   - Profile memory usage

**Deliverables:**
- âœ… Model pool with pre-warming
- âœ… Batch inference queue
- âœ… Optimized prompts
- âœ… GPU monitoring
- âœ… Benchmark results

**Acceptance Criteria:**
- Cold start <5s
- Warm inference <2s
- Throughput >20 tests/minute
- GPU utilization >60%

---

#### Milestone 3.2: Privacy Mode Implementation
**Duration:** 3 days
**Owner:** Security team
**Dependencies:** Milestone 2.2

**Tasks:**
1. Implement privacy guard
   ```typescript
   class PrivacyModeGuard {
     validate(provider: ILLMProvider): void {
       if (this.config.mode === 'strict') {
         if (!(provider instanceof RuvllmProvider)) {
           throw new PrivacyViolationError(
             'Privacy mode requires local-only inference'
           );
         }
       }
     }

     auditLog(request: LLMRequest): void {
       // Log to audit trail
       this.logger.info('LLM Request', {
         provider: request.provider,
         taskType: request.taskType,
         privacyMode: this.config.mode,
         timestamp: Date.now(),
       });
     }
   }
   ```

2. Add configuration options
   ```yaml
   # .aqe/config.yml
   privacy:
     mode: strict  # strict | balanced | off
     allowedProviders:
       - ruvllm
     auditLog: true
     blockExternalCalls: true
   ```

3. Network validation
   ```typescript
   function validateNoExternalCalls(): void {
     // Mock network layer in strict mode
     const originalFetch = global.fetch;
     global.fetch = (url: string, ...args: any[]) => {
       if (url.includes('anthropic.com') || url.includes('openai.com')) {
         throw new PrivacyViolationError(
           `Blocked external API call: ${url}`
         );
       }
       return originalFetch(url, ...args);
     };
   }
   ```

4. Generate compliance reports
   ```typescript
   class ComplianceReporter {
     async generateReport(): Promise<ComplianceReport> {
       const auditLogs = await this.loadAuditLogs();
       return {
         totalRequests: auditLogs.length,
         localRequests: auditLogs.filter(l => l.provider === 'ruvllm').length,
         cloudRequests: auditLogs.filter(l => l.provider !== 'ruvllm').length,
         violations: auditLogs.filter(l => l.blocked).length,
         certifications: ['GDPR', 'HIPAA', 'SOC2'],
       };
     }
   }
   ```

**Deliverables:**
- âœ… Privacy guard implementation
- âœ… Configuration support
- âœ… Network validation
- âœ… Compliance reporting
- âœ… Docs: `docs/guides/privacy-mode.md`

**Acceptance Criteria:**
- Strict mode blocks all cloud calls
- Audit log captures all LLM requests
- Compliance report generated successfully
- Documentation for compliance officers

---

### Phase 4: CI/CD Integration (Week 7)

#### Milestone 4.1: Docker Container Optimization
**Duration:** 3 days
**Owner:** DevOps team
**Dependencies:** Milestones 2.1, 3.1

**Tasks:**
1. Create optimized Dockerfile
   ```dockerfile
   FROM node:20-slim AS base

   # Install ruvllm system dependencies
   RUN apt-get update && apt-get install -y \
       libgomp1 \
       && rm -rf /var/lib/apt/lists/*

   # Download model during build (cached)
   RUN npx ruvllm download Qwen2.5-Coder-7B-Instruct-Q5_K_M

   WORKDIR /app
   COPY package*.json ./
   RUN npm ci --production

   COPY . .
   RUN npm run build

   # Warm up model
   RUN node -e "require('./dist/core/llm/warmup').warmUpModels()"

   EXPOSE 3000
   CMD ["aqe", "server", "--mode", "local"]
   ```

2. Multi-stage build for size optimization
   ```dockerfile
   # Stage 1: Build
   FROM node:20 AS builder
   # ... build steps

   # Stage 2: Runtime
   FROM node:20-slim
   COPY --from=builder /app/dist ./dist
   COPY --from=builder /root/.cache/ruvllm ./models
   CMD ["node", "dist/cli/index.js"]
   ```

3. GitHub Actions workflow
   ```yaml
   name: CI with Local Inference

   jobs:
     test-with-local-llm:
       runs-on: ubuntu-latest
       steps:
         - uses: actions/checkout@v3

         - name: Pull ruvllm image
           run: docker pull ghcr.io/agentic-qe/ruvllm:latest

         - name: Generate tests
           run: |
             aqe generate \
               --provider ruvllm \
               --model Qwen2.5-Coder-7B-Instruct-Q5_K_M \
               --files src/services/**/*.ts

         - name: Run tests
           run: npm test
   ```

4. Kubernetes deployment
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: aqe-local-inference
   spec:
     replicas: 3
     template:
       spec:
         containers:
         - name: aqe
           image: ghcr.io/agentic-qe/ruvllm:latest
           resources:
             requests:
               memory: "8Gi"
               cpu: "4"
             limits:
               memory: "16Gi"
               cpu: "8"
   ```

**Deliverables:**
- âœ… Optimized Dockerfile (<4GB image)
- âœ… GitHub Actions workflow
- âœ… Kubernetes manifests
- âœ… Docs: `docs/deployment/docker.md`

**Acceptance Criteria:**
- Docker image builds in <10 minutes
- Image size <4GB (compressed)
- CI pipeline runs in <5 minutes
- Kubernetes deployment stable

---

#### Milestone 4.2: CI Performance Validation
**Duration:** 2 days
**Owner:** QA team
**Dependencies:** Milestone 4.1

**Tasks:**
1. Benchmark CI pipeline
   - Measure baseline (cloud API)
   - Measure with local inference
   - Compare latency, cost, reliability

2. Load testing
   - Simulate 10 concurrent CI jobs
   - Validate throughput and memory
   - Identify bottlenecks

3. Create dashboard
   - Grafana dashboard for CI metrics
   - Track inference latency, cost savings
   - Alert on performance degradation

**Deliverables:**
- âœ… CI benchmark results
- âœ… Load test report
- âœ… Grafana dashboard
- âœ… Docs: `docs/benchmarks/ci-performance.md`

**Acceptance Criteria:**
- CI pipeline 3x faster with local inference
- Cost reduced by 70%+
- Throughput >10 jobs/minute
- Dashboard shows real-time metrics

---

### Phase 5: Quality Assurance (Week 8)

#### Milestone 5.1: Quality Validation Framework
**Duration:** 3 days
**Owner:** QA team
**Dependencies:** All previous milestones

**Tasks:**
1. Implement A/B testing framework
   ```typescript
   class QualityValidator {
     async compareProviders(
       task: QETask,
       localProvider: ILLMProvider,
       cloudProvider: ILLMProvider
     ): Promise<ComparisonResult> {
       const [localResult, cloudResult] = await Promise.all([
         localProvider.complete(task.prompt),
         cloudProvider.complete(task.prompt),
       ]);

       return {
         localOutput: localResult,
         cloudOutput: cloudResult,
         similarity: this.calculateSimilarity(localResult, cloudResult),
         metrics: {
           localLatency: 0, // measured
           cloudLatency: 0,
           localCost: 0,
           cloudCost: 0,
         },
       };
     }
   }
   ```

2. Define quality metrics
   - **Correctness:** Do generated tests compile?
   - **Coverage:** Does local achieve same coverage as cloud?
   - **Diversity:** Are edge cases still covered?
   - **Performance:** Is latency acceptable?

3. Automated regression testing
   ```typescript
   describe('Local vs Cloud Quality', () => {
     it('generates equivalent tests', async () => {
       const localTests = await localProvider.generateTests(sourceCode);
       const cloudTests = await cloudProvider.generateTests(sourceCode);

       expect(localTests.length).toBeGreaterThan(cloudTests.length * 0.8);
       expect(localTests.coverage).toBeGreaterThan(cloudTests.coverage * 0.9);
     });
   });
   ```

4. User acceptance testing
   - Run with 5 pilot users
   - Collect feedback on quality
   - Iterate on prompts if needed

**Deliverables:**
- âœ… A/B testing framework
- âœ… Quality metrics dashboard
- âœ… Regression test suite
- âœ… UAT results
- âœ… Docs: `docs/quality/validation.md`

**Acceptance Criteria:**
- Local inference achieves 90%+ quality of cloud
- No critical regressions detected
- User feedback positive (4/5 stars)
- Documentation complete

---

#### Milestone 5.2: Release Preparation
**Duration:** 2 days
**Owner:** Release team
**Dependencies:** Milestone 5.1

**Tasks:**
1. Update documentation
   - Migration guide for existing users
   - Configuration reference
   - Troubleshooting guide

2. Prepare changelog
   - Feature summary
   - Breaking changes (if any)
   - Migration steps

3. Create release artifacts
   - NPM package (agentic-qe v2.2.0)
   - Docker images
   - Binary releases (optional)

4. Write announcement blog post
   - Feature highlights
   - Performance benchmarks
   - Cost savings calculator

**Deliverables:**
- âœ… Updated docs site
- âœ… Changelog
- âœ… Release artifacts
- âœ… Blog post draft

**Acceptance Criteria:**
- Documentation complete and reviewed
- Release builds successfully
- Announcement ready

---

## Part 5: Expected Benefits and Metrics

### 5.1 Success Metrics

**Performance Metrics:**
- âœ… **Inference Latency:** <2s per request (vs 200-500ms cloud, but higher throughput)
- âœ… **Throughput:** 20+ tests/minute local vs 10 tests/minute cloud (rate limits)
- âœ… **Cold Start:** <5s model load time
- âœ… **CI Pipeline Speed:** 3x faster end-to-end (batch advantages)

**Cost Metrics:**
- âœ… **Cost Reduction:** 60-80% for users generating >1M tokens/month
- âœ… **Break-Even Point:** 500K tokens/month (infrastructure amortization)
- âœ… **TCO:** Detailed cost model in documentation

**Quality Metrics:**
- âœ… **Test Coverage:** Local achieves 90%+ of cloud coverage
- âœ… **Correctness:** 95%+ of generated tests compile and pass
- âœ… **User Satisfaction:** 4+ stars in feedback surveys

**Adoption Metrics:**
- âœ… **Opt-In Rate:** 30%+ of users enable local inference in first month
- âœ… **Privacy Mode Usage:** 10%+ of enterprise users enable strict mode
- âœ… **CI Integration:** 50%+ of CI pipelines use local inference

### 5.2 Expected Benefits

**For Developers:**
- âœ… Faster feedback loop during test-driven development
- âœ… Offline development capability (flights, remote work)
- âœ… No API key management in personal projects
- âœ… Confidence in code privacy

**For Enterprises:**
- âœ… Compliance with GDPR, HIPAA, SOC2 via privacy mode
- âœ… Predictable costs (capex vs opex)
- âœ… Air-gapped deployment support
- âœ… Custom model fine-tuning potential

**For CI/CD Pipelines:**
- âœ… 3x faster test generation
- âœ… No rate limit throttling
- âœ… Simplified secrets management
- âœ… Better parallelization

**For Open Source Projects:**
- âœ… Reduce API costs for contributors
- âœ… Enable broader community participation
- âœ… Sustainability (no ongoing cloud costs)

### 5.3 Risk Mitigation

**Technical Risks:**
- **Model Quality:** Continuous A/B testing, fallback to cloud
- **Performance:** Benchmarking, optimization sprints
- **Compatibility:** Extensive integration testing
- **GPU Availability:** CPU fallback, cloud hybrid

**Operational Risks:**
- **Support Burden:** Comprehensive docs, troubleshooting guides
- **Adoption Friction:** Gradual rollout, opt-in by default
- **Breaking Changes:** Backward compatibility, migration tools

**Business Risks:**
- **ROI Uncertainty:** Conservative estimates, pilot programs
- **Market Timing:** Align with LLM cost trends
- **Competitive Pressure:** Differentiation via privacy, performance

---

## Part 6: Next Steps

### Immediate Actions (This Week)
1. **Review & Approval:** Present plan to team, get buy-in
2. **Resource Allocation:** Assign owners to each milestone
3. **Tooling Setup:** Install ruvllm, test basic inference
4. **Prototype:** Build minimal `RuvllmProvider` (1 day spike)

### Week 1 Kickoff
1. **Phase 1 Start:** Begin LLM abstraction layer
2. **Architecture Review:** Validate design with team
3. **Documentation:** Start writing guides
4. **Communication:** Announce project to community

### Checkpoints
- **Week 2:** Phase 1 complete, demo provider switching
- **Week 4:** Phase 2 complete, demo local test generation
- **Week 6:** Phase 3 complete, benchmark results
- **Week 8:** Phase 5 complete, release candidate

### Success Criteria for Go-Live
- âœ… All milestones complete
- âœ… Quality validation passed (90%+ equivalence)
- âœ… Documentation complete
- âœ… 3 pilot users successfully onboarded
- âœ… Performance benchmarks meet targets
- âœ… Security review passed

---

## Appendix A: Technical Architecture

### A.1 Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agentic QE Fleet                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  18 QE Agents (BaseAgent)                           â”‚   â”‚
â”‚  â”‚  - qe-test-generator                                 â”‚   â”‚
â”‚  â”‚  - qe-coverage-analyzer                              â”‚   â”‚
â”‚  â”‚  - qe-flaky-test-hunter                              â”‚   â”‚
â”‚  â”‚  - ... (15 more agents)                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â”‚                                          â”‚
â”‚                   â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      HybridModelRouter (Goal 3)                      â”‚   â”‚
â”‚  â”‚  - Routing Logic: complexity, privacy, cost          â”‚   â”‚
â”‚  â”‚  - Fallback: local â†’ cloud                           â”‚   â”‚
â”‚  â”‚  - Telemetry: latency, cost, quality                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                   â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚         â–¼                    â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  ILLMProviderâ”‚â—„â”€â”€â”€â”€â”€â”¤ProviderFactory â”‚                   â”‚
â”‚  â”‚  (Goal 1)   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â–³                                                    â”‚
â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚                â”‚                â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Anthropic  â”‚  â”‚   OpenAI    â”‚  â”‚   Ruvllm    â”‚         â”‚
â”‚  â”‚  Provider   â”‚  â”‚  Provider   â”‚  â”‚  Provider   â”‚         â”‚
â”‚  â”‚  (Cloud)    â”‚  â”‚  (Cloud)    â”‚  â”‚  (Local)    â”‚         â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚  (Goal 2)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                            â”‚                 â”‚
â”‚                                            â–¼                 â”‚
â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                                  â”‚   ruvllm Core    â”‚        â”‚
â”‚                                  â”‚  - Model Pool    â”‚        â”‚
â”‚                                  â”‚  - GPU Accel     â”‚        â”‚
â”‚                                  â”‚  - Batch Queue   â”‚        â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### A.2 Sequence Diagram: Test Generation with Hybrid Routing

```
Developer          FleetCommander      HybridRouter       RuvllmProvider      AnthropicProvider
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”œâ”€ aqe generate â”€â”€â”€â”€â”€â–º                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”œâ”€ selectProvider â–º                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”œâ”€ analyze task     â”‚                     â”‚
    â”‚                     â”‚                  â”‚  (complexity,     â”‚                     â”‚
    â”‚                     â”‚                  â”‚   privacy)        â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”œâ”€ route decision   â”‚                     â”‚
    â”‚                     â”‚                  â”‚  â†’ LOCAL          â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”œâ”€ complete() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚                     â”‚                  â”‚  â”‚ Model loaded in memory   â”‚         â”‚
    â”‚                     â”‚                  â”‚  â”‚ Generate test code        â”‚         â”‚
    â”‚                     â”‚                  â”‚  â”‚ Token streaming           â”‚         â”‚
    â”‚                     â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
    â”‚                     â”‚  (test code)     â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                  â”‚                   â”‚                     â”‚
    â”‚  (test suite)       â”‚                  â”‚                   â”‚                     â”‚
    â”‚                     â”‚                  â”‚                   â”‚                     â”‚
```

---

## Appendix B: Cost Analysis

### B.1 Current State (Cloud-Only)

**Assumptions:**
- 18 agents active
- 1M tokens/day per agent (average)
- Claude Sonnet 4.5: $3.00 per 1M input tokens
- Claude Haiku: $0.25 per 1M input tokens
- Mixed usage: 70% Haiku, 30% Sonnet

**Monthly Cost:**
```
Daily tokens: 18 agents Ã— 1M tokens = 18M tokens
Monthly tokens: 18M Ã— 30 = 540M tokens

Haiku: 540M Ã— 70% Ã— $0.25/1M = $94.50
Sonnet: 540M Ã— 30% Ã— $3.00/1M = $486.00
Total: $580.50/month
```

### B.2 Hybrid State (Local + Cloud)

**Assumptions:**
- 80% of requests routed to local (test generation, pattern matching)
- 20% of requests to cloud (complex reasoning, quality gate)
- Local infrastructure: $500/month (GPU server or amortized)

**Monthly Cost:**
```
Local (80%): 432M tokens â†’ Infrastructure cost: $500
Cloud (20%): 108M tokens
  Haiku: 108M Ã— 70% Ã— $0.25/1M = $18.90
  Sonnet: 108M Ã— 30% Ã— $3.00/1M = $97.20
Total: $500 + $18.90 + $97.20 = $616.10/month
```

**Wait, that's more expensive!** ğŸ¤”

### B.3 Corrected Cost Model (Break-Even Analysis)

The key insight is that local inference only saves money at **high scale**. Let's recalculate:

**Break-Even Calculation:**
```
Fixed cost (local): $500/month
Variable cost saved: ($580.50 - $116.10) = $464.40/month

For break-even, you need usage > 2M tokens/day/agent
```

**Revised Recommendation:**
- **Low usage (<1M tokens/day):** Stay cloud-only
- **Medium usage (1-5M tokens/day):** Hybrid makes sense
- **High usage (>5M tokens/day):** Local-first saves significantly

**High Usage Example (5M tokens/day/agent):**
```
Daily tokens: 18 agents Ã— 5M = 90M tokens
Monthly tokens: 90M Ã— 30 = 2.7B tokens

Cloud-only cost:
  Haiku: 2.7B Ã— 70% Ã— $0.25/1M = $472.50
  Sonnet: 2.7B Ã— 30% Ã— $3.00/1M = $2,430.00
  Total: $2,902.50/month

Hybrid cost (80% local):
  Local: $500 (infrastructure)
  Cloud (20%): 540M tokens = $580.50
  Total: $1,080.50/month

Savings: $1,822/month (63% reduction) âœ…
```

---

## Appendix C: References

**Technical Documentation:**
- ruvllm GitHub: https://github.com/ruvllm/ruvllm
- Qwen2.5-Coder Models: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct
- AgentDB: https://github.com/ruvnet/agentdb

**Related Projects:**
- Agentic QE Fleet: https://github.com/proffesor-for-testing/agentic-qe
- Claude Flow: https://github.com/ruvnet/claude-flow

**Research Papers:**
- "Goal-Oriented Action Planning for Game AI" (Orkin, 2006)
- "Efficient Test Generation via Sublinear Optimization" (internal)

---

## Document History

| Version | Date       | Author       | Changes                          |
|---------|------------|--------------|----------------------------------|
| 1.0.0   | 2025-12-04 | Claude Code  | Initial GOAP plan created        |

---

**End of Document**

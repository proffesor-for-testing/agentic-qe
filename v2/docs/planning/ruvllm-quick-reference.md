# ruvllm Integration - Quick Reference Card

**Version:** 1.0.0 | **Status:** Planning | **Target:** v2.2.0

---

## ğŸ¯ Project Goals (1-Minute Overview)

| Goal | Description | Impact |
|------|-------------|--------|
| **G1: LLM Abstraction** | Decouple agents from specific providers | Pluggable backends |
| **G2: Local Inference** | Enable ruvllm for offline capability | 80% requests local |
| **G3: Hybrid Routing** | Smart local/cloud selection | Cost + performance |
| **G4: Privacy Mode** | Guarantee no external calls | GDPR/HIPAA compliance |
| **G5: CI/CD Optimization** | 3x faster pipelines | Developer experience |

---

## ğŸ“… Timeline (8 Weeks)

```
Week 1-2: Foundation â†’ LLM abstraction, BaseAgent refactor
Week 3-4: Core       â†’ ruvllm provider, hybrid routing
Week 5-6: Optimize   â†’ Performance, privacy mode
Week 7:   CI/CD      â†’ Docker, GitHub Actions
Week 8:   QA         â†’ Validation, release prep
```

---

## ğŸ”‘ Key Deliverables

### Phase 1 (Weeks 1-2)
- âœ… `ILLMProvider` interface
- âœ… All 18 agents use abstraction
- âœ… Can switch providers via config

### Phase 2 (Weeks 3-4)
- âœ… `RuvllmProvider` implementation
- âœ… Qwen2.5-Coder-7B model support
- âœ… Hybrid routing: 80% local, 20% cloud

### Phase 3 (Weeks 5-6)
- âœ… Model warm pool (<5s cold start)
- âœ… Batch inference (20 tests/min)
- âœ… Privacy mode (strict/balanced/off)

### Phase 4 (Week 7)
- âœ… Docker image (<4GB)
- âœ… CI pipeline 3x faster

### Phase 5 (Week 8)
- âœ… A/B testing (local vs cloud)
- âœ… Release v2.2.0

---

## ğŸ’° Cost Impact

### Break-Even Point
- **Low usage** (<1M tokens/day): Cloud cheaper
- **Medium usage** (1-5M tokens/day): Hybrid competitive
- **High usage** (>5M tokens/day): **63% savings** ($2,902 â†’ $1,080/month)

### ROI Calculator
```
Monthly savings = (Cloud cost - Hybrid cost)
Cloud cost = tokens/month Ã— mix Ã— price
Hybrid cost = $500 (infra) + 20% Ã— Cloud cost

Example (2.7B tokens/month):
Cloud: $2,902
Hybrid: $1,080
Savings: $1,822 (63%)
```

---

## ğŸ› ï¸ Technical Architecture

```
Agents (18)
    â†“
ILLMProvider interface
    â†“
HybridModelRouter
    â”œâ”€â†’ RuvllmProvider (local, 80%)
    â””â”€â†’ AnthropicProvider (cloud, 20%)
```

---

## ğŸ“Š Success Metrics

| Metric | Current | Target |
|--------|---------|--------|
| Inference latency | 500ms | <2s |
| Throughput | 10/min | 20+/min |
| Cold start | N/A | <5s |
| CI speed | 120s | <40s |
| Cost (high-vol) | $2,902 | $1,080 |
| Test coverage | 100% | 90%+ |
| Opt-in rate | N/A | 30% |

---

## ğŸ”’ Privacy Mode

### Configuration
```yaml
privacy:
  mode: strict         # strict | balanced | off
  allowedProviders: [ruvllm]
  auditLog: true
  blockExternalCalls: true
```

### Modes
- **Strict:** Local-only, no external calls
- **Balanced:** Prefer local, fallback to cloud
- **Off:** Cloud-first (current behavior)

---

## ğŸš€ Quick Start (Post-Launch)

### Install
```bash
npm install -g agentic-qe@2.2.0
```

### Configure Local Inference
```bash
# Download model (one-time)
aqe model download Qwen2.5-Coder-7B-Instruct-Q5_K_M

# Enable hybrid mode
aqe config set inference.mode hybrid

# Generate tests with local inference
aqe generate --provider ruvllm --files src/**/*.ts
```

### Enable Privacy Mode
```bash
aqe config set privacy.mode strict
aqe generate --files src/  # Guaranteed local-only
```

---

## ğŸ› Troubleshooting

### Common Issues

**Issue:** Model download fails
```bash
# Solution: Specify custom cache dir
export RUVLLM_CACHE=/path/to/cache
aqe model download <model-name>
```

**Issue:** GPU not detected
```bash
# Solution: Verify CUDA/Metal
nvidia-smi  # Linux
system_profiler SPDisplaysDataType  # macOS

# Force CPU mode
aqe config set ruvllm.gpuLayers 0
```

**Issue:** Local inference too slow
```bash
# Solution: Use smaller model
aqe model download Phi-3-Mini-4K-Instruct  # 2.3GB vs 5.6GB
aqe config set ruvllm.model Phi-3-Mini-4K-Instruct
```

**Issue:** Out of memory (OOM)
```bash
# Solution: Reduce context length
aqe config set ruvllm.contextLength 4096  # Default: 8192
```

---

## ğŸ“š Documentation Links

### User Guides
- [Getting Started with Local Inference](../guides/local-inference.md)
- [Privacy Mode Guide](../guides/privacy-mode.md)
- [CI/CD Integration](../deployment/docker.md)

### Technical Docs
- [LLM Provider Architecture](../architecture/llm-providers.md)
- [Hybrid Routing Design](../architecture/hybrid-routing.md)
- [Migration Guide](../migration/llm-providers.md)

### Planning Docs
- [Executive Summary](./ruvllm-integration-executive-summary.md)
- [Full GOAP Plan](./ruvllm-integration-goap-plan.md)
- [Visual Roadmap](./ruvllm-integration-roadmap.md)

---

## ğŸ§ª Testing Commands

### Unit Tests
```bash
npm run test:unit -- src/core/llm
```

### Integration Tests
```bash
npm run test:integration -- tests/integration/llm
```

### Benchmark
```bash
npm run test:benchmark -- benchmarks/local-vs-cloud.ts
```

### A/B Test
```bash
aqe test compare --local ruvllm --cloud anthropic --files src/
```

---

## ğŸ”¥ CI/CD Example

### GitHub Actions
```yaml
name: Tests with Local Inference

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Pull ruvllm image
        run: docker pull ghcr.io/agentic-qe/ruvllm:2.2.0

      - name: Generate tests
        run: |
          aqe generate \
            --provider ruvllm \
            --model Qwen2.5-Coder-7B-Instruct-Q5_K_M \
            --files src/**/*.ts

      - name: Run tests
        run: npm test
```

---

## ğŸ›ï¸ Configuration Reference

### Environment Variables
```bash
# Model configuration
export RUVLLM_MODEL=Qwen2.5-Coder-7B-Instruct-Q5_K_M
export RUVLLM_CACHE=/path/to/models
export RUVLLM_GPU_LAYERS=-1  # -1 = auto-detect

# Privacy mode
export AQE_PRIVACY_MODE=strict
export AQE_AUDIT_LOG=true

# Performance tuning
export RUVLLM_CONTEXT_LENGTH=8192
export RUVLLM_BATCH_SIZE=4
export RUVLLM_THREADS=8
```

### Config File (.aqe/config.yml)
```yaml
inference:
  mode: hybrid  # local | cloud | hybrid
  preferLocal: true
  fallbackToCloud: true

ruvllm:
  model: Qwen2.5-Coder-7B-Instruct-Q5_K_M
  gpuLayers: -1  # Auto-detect
  contextLength: 8192
  temperature: 0.7
  maxTokens: 2048

privacy:
  mode: strict  # strict | balanced | off
  allowedProviders: [ruvllm]
  auditLog: true
  blockExternalCalls: true

routing:
  rules:
    - name: privacy-code
      condition: privacy === 'strict'
      action: route-to-local
      priority: 100

    - name: high-complexity
      condition: complexity > 0.8
      action: route-to-cloud
      priority: 80

    - name: cost-optimization
      condition: taskType === 'test-generation'
      action: prefer-local
      priority: 50
```

---

## ğŸš¦ Status Codes

### CLI Exit Codes
- `0`: Success
- `1`: General error
- `2`: Configuration error
- `3`: Model not found
- `4`: Inference error (fallback to cloud if enabled)
- `5`: Privacy violation (strict mode)

---

## ğŸ“ Support

**Issues:** https://github.com/proffesor-for-testing/agentic-qe/issues
**Discussions:** https://github.com/proffesor-for-testing/agentic-qe/discussions
**Slack:** `#aqe-ruvllm-integration`

---

## ğŸ† Key Features (Marketing)

### For Developers
- âœ… **Offline Development:** Work on flights, remote locations
- âœ… **Faster Feedback:** 3x faster CI pipelines
- âœ… **No API Keys:** Simplified local development

### For Enterprises
- âœ… **Compliance:** GDPR, HIPAA, SOC2 via privacy mode
- âœ… **Cost Savings:** 63% reduction for high-volume users
- âœ… **Air-Gapped:** Deploy in isolated environments

### For Open Source
- âœ… **Sustainability:** No ongoing cloud costs
- âœ… **Community:** Enable broader participation
- âœ… **Privacy:** Code stays on contributor machines

---

## ğŸ”® Future Enhancements (v2.3.0+)

1. **Custom Model Fine-Tuning**
   - Train on your codebase for better accuracy
   - Enterprise tier feature

2. **Distributed Inference**
   - Cluster of GPU servers for scale
   - Kubernetes operator

3. **Model Marketplace**
   - Community-trained models
   - Domain-specific models (React, Python, etc.)

4. **Edge Deployment**
   - Run on developer laptops
   - Mobile device support (iOS/Android)

5. **Federated Learning**
   - Learn from distributed deployments
   - Privacy-preserving model updates

---

## ğŸ“‹ Checklist for New Users

### Getting Started
- [ ] Install `agentic-qe@2.2.0`
- [ ] Download model: `aqe model download Qwen2.5-Coder-7B-Instruct-Q5_K_M`
- [ ] Configure hybrid mode: `aqe config set inference.mode hybrid`
- [ ] Test generation: `aqe generate --provider ruvllm --files src/`
- [ ] Validate: `npm test`

### For Enterprise (Privacy Mode)
- [ ] Review privacy policy requirements (GDPR, HIPAA, etc.)
- [ ] Enable strict mode: `aqe config set privacy.mode strict`
- [ ] Configure audit logging: `aqe config set privacy.auditLog true`
- [ ] Test with sample code
- [ ] Generate compliance report: `aqe privacy report`
- [ ] Review with legal/compliance team

### For CI/CD
- [ ] Build Docker image: `docker build -t aqe-local:latest .`
- [ ] Update GitHub Actions workflow
- [ ] Test in CI: Run a pilot pipeline
- [ ] Benchmark: Compare speed (baseline vs local)
- [ ] Rollout: Enable for all pipelines

---

## ğŸ‰ Celebration Milestones

- [ ] Week 2: LLM abstraction complete ğŸŠ
- [ ] Week 4: Local inference working ğŸš€
- [ ] Week 6: Performance targets met âš¡
- [ ] Week 8: Release v2.2.0 shipped ğŸ‰
- [ ] Month 3: 30% adoption reached ğŸ†

---

**Print this card and keep it handy!**

**Last Updated:** 2025-12-04
**Version:** 1.0.0

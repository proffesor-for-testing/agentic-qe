# Local-First Deployment Mode
# Prioritizes local providers (Ollama, ruvLLM) with hosted fallback
# Ideal for: Development, cost-sensitive environments, privacy-focused deployments

mode: local_first

# Provider configuration
providers:
  # Primary: Ollama (local inference)
  - type: ollama
    enabled: true
    priority: 10  # Highest priority
    baseUrl: http://localhost:11434
    defaultModel: llama3.2:3b

    # Model selection by task type
    modelOverrides:
      test-generation: qwen2.5-coder:7b
      code-review: deepseek-coder:6.7b
      documentation: llama3.2:3b
      refactoring: qwen2.5-coder:7b
      coverage-analysis: llama3.2:3b

    # Ollama is free (local)
    costPer1MTokens:
      input: 0
      output: 0

    # Fallback to OpenRouter if Ollama fails
    fallbackProvider: openrouter

  # Secondary: OpenRouter (hosted fallback)
  - type: openrouter
    enabled: true
    priority: 20
    apiKey: ${OPENROUTER_API_KEY}
    baseUrl: https://openrouter.ai/api/v1
    defaultModel: google/gemini-flash-1.5-8b

    # Use cost-effective models
    modelOverrides:
      test-generation: google/gemini-flash-1.5-8b  # Free tier
      code-review: anthropic/claude-3.5-haiku
      documentation: google/gemini-flash-1.5-8b
      coverage-analysis: google/gemini-flash-1.5-8b

    costPer1MTokens:
      input: 0.075
      output: 0.30

    # Rate limits for free tier
    limits:
      requestsPerMinute: 20
      requestsPerDay: 200

# Fallback chain: Ollama -> OpenRouter
fallbackChain:
  - ollama
  - openrouter

# Cost budget (optional)
costBudget:
  daily: 5.0     # $5/day limit
  monthly: 100.0 # $100/month limit
  warnThreshold: 80  # Warn at 80%
  enforceLimit: true

# Health monitoring
enableHealthChecks: true
healthCheckInterval: 60000  # 1 minute
maxConsecutiveFailures: 3

# Retry configuration
enableRetries: true
maxRetries: 2

# Cost tracking
enableCostTracking: true

# Logging
logLevel: info
